data:
  train_files: train_data.parquet
  train_batch_size: 144  # 12 prompts * 12 rollouts
  max_prompt_length: 1536
  max_response_length: 1536

actor_rollout_ref:
  model:
    path: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
    dtype: bfloat16
    enable_gradient_checkpointing: true
  
  actor:
    optim:
      lr: 1e-4
      weight_decay: 0.01
    
    ppo_epochs: 8
    ppo_mini_batch_size: 12
    gradient_accumulation_steps: 2
    
    # GRPO requires KL regularization
    use_kl_loss: true
    kl_loss_coef: 0.001
    kl_loss_type: kl  # Options: kl, abs, mse, low_var_kl, full
    gradient_clip: 1.0
    loss_agg_mode: token-mean  # Default for GRPO
    
  rollout:
    n: 12  # Must be > 1 for GRPO group sampling
    temperature: 1.0
    top_p: 0.95
    use_vllm: true

algorithm:
  adv_estimator: grpo  # Required for GRPO
  use_kl_in_reward: false  # GRPO uses KL in loss, not reward

trainer:
  total_epochs: 8
  save_freq: 10
  test_freq: 5
  logger: ["console", "wandb"]
  seed: 42

reward_function:
  path: exploration_hacking.verl_reward_wrapper
  name: load_reward_function
  kwargs:
    pickle_path: reward_data.pkl